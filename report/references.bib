@article{weyssow2024codeultrafeedback,
  title={CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences},
  author={Weyssow, Martin and Kamanda, Aton and Zhou, Xin and Sahraoui, Houari},
  journal={arXiv preprint arXiv:2403.09032},
  year={2024}
}

@inproceedings{koutcheme2025evaluating,
  title={Evaluating Language Models for Generating and Judging Programming Feedback},
  author={Koutcheme, Charles and Dainese, Nicola and Sarsa, Sami and Hellas, Arto and Leinonen, Juho and Ashraf, Syed and Denny, Paul},
  booktitle={Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
  pages={624--630},
  year={2025}
}

@article{zhao2024codejudgeeval,
  title={CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?},
  author={Zhao, Yuwei and Luo, Ziyang and Tian, Yuchen and Lin, Hongzhan and Yan, Weixiang and Li, Annan and Ma, Jing},
  journal={arXiv preprint arXiv:2408.10718},
  year={2024}
}

@article{west2023generative,
  title={The Generative AI Paradox: "What It Can Create, It May Not Understand"},
  author={West, Peter and Lu, Ximing and Dziri, Nouha and Brahman, Faeze and Li, Linjie and Hwang, Jena D and Jiang, Liwei and Fisher, Jillian and Ravichander, Abhilasha and Chandu, Khyathi Raghavi and others},
  journal={arXiv preprint arXiv:2311.00059},
  year={2023}
}

@article{gu2024counterfeit,
  title={The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?},
  author={Gu, Alex and Li, Wen-Ding and Jain, Naman and Olausson, Theo X and Lee, Charles and Sen, Koushik and Solar-Lezama, Armando},
  journal={arXiv preprint arXiv:2402.19475},
  year={2024}
}

@article{tong2024codejudge,
  title={CodeJudge: Evaluating Code Generation with Large Language Models},
  author={Tong, Weixi and Zhang, Tianyi},
  journal={arXiv preprint arXiv:2410.02184},
  year={2024}
}

@article{zhuo2023icescore,
  title={ICE-Score: Instructing Large Language Models to Evaluate Code},
  author={Zhuo, Terry Yue},
  journal={arXiv preprint arXiv:2304.14317},
  year={2023}
}

@misc{openai2023chatgpt,
  title={ChatGPT},
  author={OpenAI},
  year={2023},
  howpublished={\url{https://openai.com/chatgpt}}
}

@misc{githubcopilot,
  title={GitHub Copilot},
  author={{GitHub}},
  year={2023},
  howpublished={\url{https://github.com/features/copilot}}
}

@article{li2023starcoder,
  title={StarCoder: May the source be with you!},
  author={Li, Yizhong and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023}
}

@article{jiang2024survey,
  title={A Survey on Large Language Models for Code Generation},
  author={Jiang, Jue and others},
  journal={arXiv preprint arXiv:2406.00515},
  year={2024}
}

@article{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{wang2023codet,
  title={CodeT: Code Generation with Generated Tests},
  author={Wang, Pengcheng and others},
  journal={arXiv preprint arXiv:2305.14278},
  year={2023}
}

@inproceedings{papineni2002bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and others},
  booktitle={Proceedings of ACL},
  year={2002}
}

@inproceedings{lin2004rouge,
  title={ROUGE: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text Summarization Branches Out},
  year={2004}
}

@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={ACL Workshop},
  year={2005}
}

@article{zhang2019bertscore,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Zhang, Tianyi and others},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}

@article{naik2024limitations,
  title={On the Limitations of Embedding Based Methods for Measuring Functional Correctness for Code Generation},
  author={Naik, Aniruddha},
  journal={arXiv preprint arXiv:2405.01580},
  year={2024}
}

@article{lzi2023humanevalplus,
  title={HumanEval+: A more thorough benchmark for evaluating the generalizability of code generation models},
  author={Li, Ziyi and Zhao, Yuwei and Zhao, Yujia and Liu, Yao and Zeng, Zhi and Liu, Yin and Sun, Maosong},
  journal={arXiv preprint arXiv:2307.13859},
  year={2023}
}