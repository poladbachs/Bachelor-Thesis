@article{weyssow2024codeultrafeedback,
  title={CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences},
  author={Weyssow, Martin and Kamanda, Aton and Zhou, Xin and Sahraoui, Houari},
  journal={arXiv preprint arXiv:2403.09032},
  year={2024}
}

@inproceedings{koutcheme2025evaluating,
  title={Evaluating Language Models for Generating and Judging Programming Feedback},
  author={Koutcheme, Charles and Dainese, Nicola and Sarsa, Sami and Hellas, Arto and Leinonen, Juho and Ashraf, Syed and Denny, Paul},
  booktitle={Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
  pages={624--630},
  year={2025}
}

@article{zhao2024codejudgeeval,
  title={CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?},
  author={Zhao, Yuwei and Luo, Ziyang and Tian, Yuchen and Lin, Hongzhan and Yan, Weixiang and Li, Annan and Ma, Jing},
  journal={arXiv preprint arXiv:2408.10718},
  year={2024}
}

@article{west2023generative,
  title={The Generative AI Paradox: "What It Can Create, It May Not Understand"},
  author={West, Peter and Lu, Ximing and Dziri, Nouha and Brahman, Faeze and Li, Linjie and Hwang, Jena D and Jiang, Liwei and Fisher, Jillian and Ravichander, Abhilasha and Chandu, Khyathi Raghavi and others},
  journal={arXiv preprint arXiv:2311.00059},
  year={2023}
}

@article{gu2024counterfeit,
  title={The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?},
  author={Gu, Alex and Li, Wen-Ding and Jain, Naman and Olausson, Theo X and Lee, Charles and Sen, Koushik and Solar-Lezama, Armando},
  journal={arXiv preprint arXiv:2402.19475},
  year={2024}
}

@article{tong2024codejudge,
  title={CodeJudge: Evaluating Code Generation with Large Language Models},
  author={Tong, Weixi and Zhang, Tianyi},
  journal={arXiv preprint arXiv:2410.02184},
  year={2024}
}

@article{zhuo2023icescore,
  title={ICE-Score: Instructing Large Language Models to Evaluate Code},
  author={Zhuo, Terry Yue},
  journal={arXiv preprint arXiv:2304.14317},
  year={2023}
}

@misc{openai2023chatgpt,
  title={ChatGPT},
  author={OpenAI},
  year={2023},
  howpublished={\url{https://openai.com/chatgpt}}
}

@misc{githubcopilot,
  title={GitHub Copilot},
  author={{GitHub}},
  year={2023},
  howpublished={\url{https://github.com/features/copilot}}
}

@article{li2023starcoder,
  title={StarCoder: May the source be with you!},
  author={Li, Yizhong and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023}
}

@article{jiang2024survey,
  title={A Survey on Large Language Models for Code Generation},
  author={Jiang, Jue and others},
  journal={arXiv preprint arXiv:2406.00515},
  year={2024}
}

@article{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{wang2023codet,
  title={CodeT: Code Generation with Generated Tests},
  author={Wang, Pengcheng and others},
  journal={arXiv preprint arXiv:2305.14278},
  year={2023}
}

@inproceedings{papineni2002bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and others},
  booktitle={Proceedings of ACL},
  year={2002}
}

@inproceedings{lin2004rouge,
  title={ROUGE: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text Summarization Branches Out},
  year={2004}
}

@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={ACL Workshop},
  year={2005}
}

@article{zhang2019bertscore,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Zhang, Tianyi and others},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}

@article{naik2024limitations,
  title={On the Limitations of Embedding Based Methods for Measuring Functional Correctness for Code Generation},
  author={Naik, Aniruddha},
  journal={arXiv preprint arXiv:2405.01580},
  year={2024}
}

@article{lzi2023humanevalplus,
  title={HumanEval+: A more thorough benchmark for evaluating the generalizability of code generation models},
  author={Li, Ziyi and Zhao, Yuwei and Zhao, Yujia and Liu, Yao and Zeng, Zhi and Liu, Yin and Sun, Maosong},
  journal={arXiv preprint arXiv:2307.13859},
  year={2023}
}

@article{qwen2024report,
  title={Qwen2.5: Scaling up Language Models with Data and Instruction Tuning},
  author={Qwen Team and Alibaba DAMO Academy},
  journal={arXiv preprint arXiv:2403.16880},
  year={2024},
  url={https://arxiv.org/abs/2403.16880}
}

@article{deepseek2024report,
  title={DeepSeek-Coder: When the Large Language Model Meets Programming – The Rise of Code Intelligence},
  author={DeepSeek AI},
  journal={arXiv preprint arXiv:2401.16670},
  year={2024},
  url={https://arxiv.org/abs/2401.16670}
}

@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-Art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clément Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander Rush},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={38--45},
  year={2020},
  organization={Association for Computational Linguistics},
  url={https://github.com/huggingface/transformers}
}

@misc{coderEval2023,
  title         = {CoderEval Benchmark},
  author        = {{OpenBMB}},
  year          = {2023},
  howpublished  = {\url{https://github.com/CoderEval/CoderEval}},
  note          = {Java subset used as base for this study}
}

@article{Lu2021codexglue,
  title={CodeXGLUE: A Benchmark Dataset and Open Challenge for Code Intelligence},
  author={Lu, Shuai and Guo, Daya and Ren, Shuo and Tang, Duyu and Duan, Nan and Zhou, Ming and Jiang, Daxin},
  journal={arXiv preprint arXiv:2102.04664},
  year={2021},
  url={https://arxiv.org/abs/2102.04664}
}

@inproceedings{smith2021human,
  title={Human-in-the-loop evaluation of AI-generated code: Design and implications},
  author={Smith, Emily and Williams, Laurie},
  booktitle={Proceedings of the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  year={2021},
  publisher={ACM}
}

@misc{claude2023blog,
  title = {Introducing Claude},
  author = {Anthropic},
  year = {2023},
  note = {Accessed May 2025},
  url = {https://www.anthropic.com/index/introducing-claude}
}